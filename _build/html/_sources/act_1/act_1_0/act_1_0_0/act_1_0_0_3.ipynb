{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# 10\n",
    "\n",
    "if you were trained on 1.3 trillion parameters, can we call them 1.3 trillion tokens?\n",
    "\n",
    "---\n",
    "\n",
    "The terms \"parameters\" and \"tokens\" refer to different concepts in the context of machine learning and natural language processing, so they're not interchangeable.\n",
    "\n",
    "1. **Parameters**: In machine learning models like GPT-4, parameters are the parts of the model that are learned from the training data. These include weights that adjust themselves during training to minimize the difference between the model's predictions and actual outcomes. In the case of GPT-4, there are approximately 1.3 trillion adjustable parameters.\n",
    "\n",
    "2. **Tokens**: In natural language processing, a token typically refers to a piece of a text that has been extracted from a larger corpus during the process of tokenization. Tokens can be as small as characters or as long as words. In the context of machine learning models, tokens are the input data points that the model processes.\n",
    "\n",
    "So, while I have 1.3 trillion parameters, these aren't tokens. The parameters are what enable me to understand and generate text based on the tokens that are input to me.\n",
    "\n",
    "---\n",
    "\n",
    "so basically the tokens are several orders of magnitude greater than 1.3 trillion?\n",
    "\n",
    "---\n",
    "\n",
    "Yes, the number of tokens that could theoretically be processed or generated is vastly greater than the 1.3 trillion parameters in the model. While the model itself has a fixed architecture with 1.3 trillion parameters that define its behavior, the input and output tokens are dynamic. They depend on the data being processed and can range across an extremely large set of possible text combinations.\n",
    "\n",
    "In the training phase, the model may have seen many trillions of tokens from its training data. However, even this large dataset is just a fraction of the possible combinations of tokens that the model could encounter or generate.\n",
    "\n",
    "So, in summary, the \"space\" of possible tokens and token combinations that could be processed or generated is several orders of magnitude larger than the 1.3 trillion parameters that make up the model.\n",
    "\n",
    "# 11\n",
    "\n",
    "**Automation, Iteration, and Token Quantification: Lessons from Icons in Clinical Research**\n",
    "\n",
    "In the realms of clinical research, where precision and excellence are paramount, the wisdom of industry icons like Chesley Sullenberger, Peter Pronovost, and Marty Makary holds particular significance. These luminaries have not only left indelible marks on their respective fields but have also showcased the enduring relevance of automation, iteration, and token quantification in achieving exceptional outcomes.\n",
    "\n",
    "**Chesley Sullenberger: Precision in the Skies**\n",
    "\n",
    "Captain Chesley \"Sully\" Sullenberger, celebrated for his remarkable \"Miracle on the Hudson,\" exemplifies the critical role of precision in aviation. Beyond his heroic water landing, Sully's commitment to safety underscores the importance of automation. In aviation, where the margin for error is slim, automation is a linchpin. Sophisticated autopilot systems and flight management computers ensure that aircraft navigate with unparalleled precision, minimizing the potential for human error.\n",
    "\n",
    "Yet, Sully's story also teaches us the value of iteration. Pilots undergo rigorous, iterative training to sharpen their skills and adapt to evolving technologies. Each flight is a learning experience, and continuous improvement is a non-negotiable ethos in aviation. The lessons from Sully's iconic landing extend beyond the cockpit, emphasizing that iteration is not confined to one profession but is a universal pursuit of excellence.\n",
    "\n",
    "**Peter Pronovost and Marty Makary: Checklist Champions**\n",
    "\n",
    "In the world of clinical research and healthcare, Peter Pronovost and Marty Makary are renowned for their groundbreaking work on checklists. Their efforts have saved countless lives by reducing medical errors and enhancing patient safety. Their checklist methodology embodies the principles of automation and token quantification.\n",
    "\n",
    "Checklists automate critical steps in complex medical procedures, ensuring that healthcare professionals adhere to best practices. These checklists act as safeguards against error, promoting precision in high-stakes environments. Moreover, they exemplify token quantification, as each item on the checklist represents a crucial token in the process. By quantifying these tokens, Pronovost and Makary provide a clear roadmap for achieving precision in healthcare.\n",
    "\n",
    "**Token Quantification: A Universal Approach**\n",
    "\n",
    "Incorporating token quantification into clinical research can lead to more rigorous and error-free studies. By quantifying the tokens within research papers, scientists can ensure that their work aligns with industry standards. Just as checklists in healthcare represent essential tokens for patient safety, token quantification acts as a checklist for precision in research.\n",
    "\n",
    "Automation, iteration, and token quantification, as demonstrated by these icons, transcend specific professions. They are universal principles that underpin excellence. Whether you're navigating a commercial airliner, conducting life-saving surgery, or conducting clinical research, these principles serve as beacons of precision. By heeding the lessons of Sully, Pronovost, and Makary, professionals in any field can chart a course toward ever-higher levels of excellence and precision.\n",
    "\n",
    "# 12\n",
    "\n",
    "**Automating Excellence: A Quantitative Approach to Tokens, Iteration, and Precision**\n",
    "\n",
    "In the ever-evolving landscape of professional disciplines, automation and iteration have become pivotal elements for achieving excellence. These concepts are not confined to any specific field but permeate various professions, including clinical research, aviation, and code development. Moreover, the quantification of tokens, with a comparative reference to median and percentile benchmarks, serves as a valuable checklist, guiding professionals in optimizing their workflows. In this essay, we will explore how automation, iteration, and token quantification can enhance precision and efficiency in any profession, emphasizing their relevance and application across diverse domains.\n",
    "\n",
    "**Automation: Streamlining Precision**\n",
    "\n",
    "Automation is the process of replacing manual tasks with efficient, computerized systems. Whether in clinical research, aviation, or code development, automation plays a critical role in streamlining processes, reducing errors, and enhancing precision. For instance, in clinical research, automation can facilitate data collection, analysis, and reporting, ensuring consistency and reliability across the study. Similarly, in aviation, automated navigation systems contribute to flight safety by executing precise maneuvers with minimal margin for error.\n",
    "\n",
    "**Iteration: The Path to Perfection**\n",
    "\n",
    "Iteration is the relentless pursuit of improvement through repeated cycles of refinement. It is a fundamental concept in fields like clinical research, where protocols and methodologies are continuously fine-tuned to achieve greater precision and reliability. Aviation professionals engage in iterative training to enhance their skills and adapt to evolving technologies. Code developers iterate through multiple versions of their software to eliminate bugs and optimize performance. Iteration is the vehicle through which professionals inch closer to perfection, honing their craft and pushing the boundaries of excellence.\n",
    "\n",
    "**Token Quantification: A Checklist for Precision**\n",
    "\n",
    "Token quantification, measuring the number of linguistic or data units in a given context, is a powerful tool for professionals seeking to optimize their workflows. It provides a quantitative benchmark against which one can assess the complexity and efficiency of their work. For example, a clinical researcher may quantify the tokens in their research paper, comparing it to the median and 25th percentile values in their field. This comparison serves as a checklist, highlighting areas where the paper may be too complex or too concise relative to the norm.\n",
    "\n",
    "Similarly, a code developer can count the tokens in their script and compare it to industry standards. If the script contains an excessive number of tokens compared to the median, it may indicate inefficiencies that need to be addressed through automation or code optimization. Conversely, a script with too few tokens may lack the necessary complexity to perform its intended function effectively.\n",
    "\n",
    "**Conclusion: Precision Through Quantitative Excellence**\n",
    "\n",
    "In any profession, automation, iteration, and token quantification are essential components of achieving precision and excellence. Automation streamlines processes, reducing errors and enhancing efficiency. Iteration drives continuous improvement, pushing the boundaries of what is possible. Token quantification serves as a quantitative checklist, guiding professionals in optimizing their work and ensuring that it aligns with industry standards.\n",
    "\n",
    "Whether you're a clinical researcher striving for rigor, an aviation professional prioritizing safety, or a code developer seeking efficiency, these principles are universal. By embracing automation, iteration, and token quantification, professionals can not only meet but exceed the benchmarks of their fields, setting new standards of precision and excellence in their chosen endeavors.\n",
    "\n",
    "# 13 \n",
    "\n",
    "**Tokens of Excellence in Clinical Research: Drawing from Aviation Icons and Checklist Innovators**\n",
    "\n",
    "In the world of clinical research, where precision and accuracy are paramount, there are invaluable lessons to be learned from aviation icons like Chesley Sullenberger and medical pioneers like Marty Makary and Peter Pronovost, renowned for their groundbreaking work on checklists in anesthesia and surgery. These remarkable individuals have left an indelible mark on their respective fields, emphasizing the importance of meticulous planning, adherence to protocols, and relentless pursuit of excellence. In this essay, we will explore how the concepts of tokens, rigor, error, and sloppiness intersect with the extraordinary careers of these aviation and medical luminaries, and how their insights can elevate the standards of clinical research to new heights.\n",
    "\n",
    "**Tokens: Precision Instruments of Excellence**\n",
    "\n",
    "Tokens in clinical research transcend their role as mere components; they are the bedrock of precision, diligence, and dependability within the research process. Much like how Chesley Sullenberger meticulously checked each instrument before takeoff, clinical researchers scrutinize and validate each token to ensure the integrity of their work. These tokens are the building blocks of data, analysis, and interpretation, representing not just linguistic or data units but precision tools meticulously honed to sculpt each facet of the clinical researcher's craft.\n",
    "\n",
    "**Rigor: Upholding Excellence through Discipline**\n",
    "\n",
    "Rigor serves as the first line of defense against sloppiness, whether in aviation or clinical research. Just as Chesley Sullenberger's unwavering discipline in adhering to flight protocols ensured the safe landing of US Airways Flight 1549, clinical researchers employ rigor to maintain the highest standards of excellence. Rigor in clinical research means painstaking attention to detail, regular audits, and a culture of hyper-vigilance. It builds a shield that sloppiness cannot easily penetrate, ensuring that every aspect of the research is conducted with meticulous care.\n",
    "\n",
    "**Error: Learning from Setbacks**\n",
    "\n",
    "In the aviation industry, errors are meticulously reviewed and analyzed to prevent future mishaps. Chesley Sullenberger's \"Miracle on the Hudson\" was the result of years of error analysis and preparedness training. Similarly, clinical researchers recognize that errors, whether in data collection, analysis, or interpretation, are opportunities for growth and improvement. By treating errors as valuable lessons, researchers ensure that they continuously refine and optimize their tokens, enhancing precision and making them resistant to sloppiness.\n",
    "\n",
    "**Sloppiness: The Adversary to Defeat**\n",
    "\n",
    "Sloppiness is the common enemy of both aviation and clinical research. In the aviation industry, the consequences of imprecise actions in the cockpit can be catastrophic. In clinical research, sloppiness can compromise the integrity of the study and erode public trust. However, just as aviation professionals employ a proactive offense to outsmart sloppiness, clinical researchers can leverage cutting-edge tools and best practices to minimize its intrusion. Tokens serve as the focal point for auditing and review processes in clinical research, ensuring that sloppiness is curbed before it impacts the final research output.\n",
    "\n",
    "**Checklists: The Paragon of Precision in Medicine**\n",
    "\n",
    "In the realm of medical innovation, Marty Makary and Peter Pronovost are renowned for their groundbreaking work on checklists in anesthesia and surgery. These checklists serve as tokens of precision and adherence to protocol in high-stakes medical procedures. They are meticulously designed to ensure that no detail is overlooked, and they have revolutionized patient safety in healthcare settings.\n",
    "\n",
    "**Conclusion: Soaring to New Heights of Excellence**\n",
    "\n",
    "In the spirit of aviation icons like Chesley Sullenberger and medical pioneers like Marty Makary and Peter Pronovost, clinical researchers are poised to soar to new heights of excellence. Tokens, rigor, error analysis, and the relentless pursuit of precision are the cornerstones of their success. As the clinical research community embraces these principles with the same unwavering commitment as these aviation and medical legends, we can expect research outcomes that are not just reliable but transformative. Through the lens of aviation excellence and medical precision, we can chart a course toward a future where every clinical researcher becomes a stalwart guardian of integrity, ensuring that their work stands as a testament to rigor and precision in the pursuit of better healthcare outcomes.\n",
    "\n",
    "# `14`\n",
    "\n",
    "**Tokens of Excellence in Clinical Research: Learning from Aviation Icons**\n",
    "\n",
    "In the realm of clinical research, where precision and accuracy are paramount, there are invaluable lessons to be gleaned from the world of aviation. Icons like Chesley Sullenberger, Marty Makary, and Peter Pronovost have left an indelible mark on aviation safety and innovation. Their dedication to meticulous planning, adherence to protocols, and relentless pursuit of excellence align seamlessly with the principles that underpin clinical research. In this essay, we'll explore how the concepts of tokens, rigor, error, and sloppiness intersect with the remarkable careers of these aviation legends and how their insights can elevate the standards of clinical research to new heights.\n",
    "\n",
    "**Tokens: Precision Instruments of Excellence**\n",
    "\n",
    "Tokens in clinical research transcend their role as mere components; they are the bedrock of precision, diligence, and dependability within the research process. Much like how Chesley Sullenberger meticulously checked each instrument before takeoff, clinical researchers scrutinize and validate each token to ensure the integrity of their work. These tokens are the building blocks of data, analysis, and interpretation, representing not just linguistic or data units but precision tools meticulously honed to sculpt each facet of the clinical researcher's craft.\n",
    "\n",
    "**Rigor: Upholding Excellence through Discipline**\n",
    "\n",
    "Rigor serves as the first line of defense against sloppiness, whether in aviation or clinical research. Just as Chesley Sullenberger's unwavering discipline in adhering to flight protocols ensured the safe landing of US Airways Flight 1549, clinical researchers employ rigor to maintain the highest standards of excellence. Rigor in clinical research means painstaking attention to detail, regular audits, and a culture of hyper-vigilance. It builds a shield that sloppiness cannot easily penetrate, ensuring that every aspect of the research is conducted with meticulous care.\n",
    "\n",
    "**Error: Learning from Setbacks**\n",
    "\n",
    "In the aviation industry, errors are meticulously reviewed and analyzed to prevent future mishaps. Chesley Sullenberger's \"Miracle on the Hudson\" was the result of years of error analysis and preparedness training. Similarly, clinical researchers recognize that errors, whether in data collection, analysis, or interpretation, are opportunities for growth and improvement. By treating errors as valuable lessons, researchers ensure that they continuously refine and optimize their tokens, enhancing precision and making them resistant to sloppiness.\n",
    "\n",
    "**Sloppiness: The Adversary to Defeat**\n",
    "\n",
    "Sloppiness is the common enemy of both aviation and clinical research. In the aviation industry, the consequences of imprecise actions in the cockpit can be catastrophic. In clinical research, sloppiness can compromise the integrity of the study and erode public trust. However, just as aviation professionals employ a proactive offense to outsmart sloppiness, clinical researchers can leverage cutting-edge tools and best practices to minimize its intrusion. Tokens serve as the focal point for auditing and review processes in clinical research, ensuring that sloppiness is curbed before it impacts the final research output.\n",
    "\n",
    "**Conclusion: Soaring to New Heights of Excellence**\n",
    "\n",
    "In the spirit of aviation icons like Chesley Sullenberger, Marty Makary, and Peter Pronovost, clinical researchers are poised to soar to new heights of excellence. Tokens, rigor, error analysis, and the relentless pursuit of precision are the cornerstones of their success. As the clinical research community embraces these principles with the same unwavering commitment as these aviation legends, we can expect research outcomes that are not just reliable but transformative. Through the lens of aviation excellence, we can chart a course toward a future where every clinical researcher becomes a stalwart guardian of integrity, ensuring that their work stands as a testament to rigor and precision in the pursuit of better healthcare outcomes.\n",
    "\n",
    "# 15\n",
    "\n",
    "In the realm of professionals striving for error reduction and the elimination of sloppiness from their workflow, the concept of tokens takes on profound significance. Tokens transcend their role as mere linguistic or data units; they symbolize the bedrock of precision, diligence, and dependability within their work. Tokens represent an opportunity to create meticulous, exacting outputs and serve as a safeguard against the specter of sloppiness.\n",
    "\n",
    "Tokens, in this context, are more than mere components; they're precision tools meticulously honed to sculpt each facet of the professional's craft. Similar to artisans relying on exact instruments for intricate creations, these tokens empower professionals to craft their work with meticulous precision. Each token, whether it's a data point, a code segment, or a word in a research document, carries the weight of precision and the potential to combat sloppiness.\n",
    "\n",
    "Furthermore, tokens instill a sense of accountability in the workflow. They allow for scrutiny and traceability. Each token can be tracked back to its origin, imbuing a sense of responsibility in the professional. This accountability acts as a bulwark against sloppiness, as the professional is aware that each token may undergo inspection by peers, reviewers, or themselves during rigorous self-audits.\n",
    "\n",
    "Tokens introduce an economy of rigor within the professional's workflow. In this token-based economy, every token expended must yield value, akin to currency in an exchange. Sloppiness devalues tokens by introducing errors and inaccuracies, diminishing the worth of the work. Hence, the professional strategically invests tokens to maximize precision and minimize the intrusion of sloppiness.\n",
    "\n",
    "Tokens also serve as the focal point for auditing and review processes. Analogous to financial transactions scrutinized by auditors, professionals can audit their work by examining each token. This approach facilitates systematic error identification and correction, ensuring that sloppiness is curbed before it permeates the final output.\n",
    "\n",
    "Tokens are not static entities; they are dynamic, subject to evolution. Professionals continuously refine and optimize their tokens, enhancing precision and making them resistant to sloppiness. This process mirrors the artisan's practice of sharpening tools for improved craftsmanship.\n",
    "\n",
    "Efficiency and innovation are also inherent in tokens. Professionals seek innovative ways to streamline their work by optimizing tokens. This may encompass automation, advanced tools, or the embrace of best practices, all of which enhance precision while reducing the risk of sloppiness.\n",
    "\n",
    "In interdisciplinary or collaborative settings, tokens act as a lingua franca. Diverse professionals align their tokens to ensure consistency and accuracy in shared projects. This collaborative token-driven approach cultivates a culture of collective precision, minimizing individual sloppiness's impact.\n",
    "\n",
    "Tokens function as gatekeepers against errors. By meticulously examining and validating each token, professionals prevent errors from proliferating through their work. This proactive approach to error prevention serves as a critical defense against sloppiness.\n",
    "\n",
    "In conclusion, tokens within a professional's pursuit of error reduction and sloppiness elimination epitomize precision, accountability, and relentless improvement. They signify not just data or language but also an unwavering commitment to excellence and a proactive stance against sloppiness. In this professional's universe, tokens represent the cornerstone of a robust defense against the pitfalls of haste and neglect, ensuring that their work stands as a testament to rigor and integrity.\n",
    "\n",
    "Now, let's consider the question of whether a professional should publish a catalog or at least a numerical reference. By doing so, they provide transparency regarding the number of tokens used in their work. This transparency can serve several purposes:\n",
    "\n",
    "1. **Benchmarking and Comparison:** Publishing a catalog or numerical reference allows professionals to benchmark their token usage against industry or field standards. They can compare their token count to median values or percentiles within their profession. This information provides valuable insights into whether their workflow is within the expected range of token usage.\n",
    "\n",
    "2. **Self-Improvement:** Knowing where one stands in terms of token usage can be a catalyst for self-improvement. Professionals can set targets to reduce token count if they find themselves consistently using more tokens than their peers. This encourages a more efficient and precise approach to work.\n",
    "\n",
    "3. **Quality Assurance:** Token counts can act as an additional quality assurance metric. If a professional consistently uses significantly fewer tokens than the median or 25% of their peers, it may raise questions about the completeness or depth of their work. Conversely, excessive token usage might indicate unnecessary verbosity.\n",
    "\n",
    "4. **Peer Collaboration:** Sharing token counts can facilitate collaboration with peers. It provides a clear understanding of the scope and complexity of a project. This transparency can help team members allocate resources and responsibilities effectively.\n",
    "\n",
    "Incorporating token counts into professional practices fosters a culture of transparency, efficiency, and continuous improvement. It allows professionals to navigate the delicate balance between precision and efficiency while keeping sloppiness at bay. Ultimately, whether to publish a catalog or numerical reference of token usage depends on the specific field and the extent to which token count serves as a meaningful metric for quality and efficiency.\n",
    "\n",
    "# 16\n",
    "\n",
    "Certainly, I can help translate your Stata script to R code. Below is the equivalent R code that should accomplish the same tasks you've laid out in the Stata script.\n",
    "\n",
    "Firstly, you would need to install and load the required R packages if you haven't already:\n",
    "\n",
    "```R\n",
    "install.packages(c(\"data.table\", \"dplyr\", \"ggplot2\", \"lme4\"))\n",
    "library(data.table)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(lme4)\n",
    "```\n",
    "\n",
    "Now, let's move on to the script.\n",
    "\n",
    "```R\n",
    "# Document session using .log file\n",
    "sink(\"poisson.log\")\n",
    "\n",
    "# Setting timeout to 10000 milliseconds\n",
    "options(timeout = 10000)\n",
    "\n",
    "# Comment indicating external data source\n",
    "repo <- \"https://github.com/muzaale/got/raw/main/act_5/act_5_9/\"\n",
    "\n",
    "# Actual loading of data\n",
    "data <- fread(paste0(repo, \"donor_live_keep.dta\"))\n",
    "\n",
    "# Change directory and load the dataset\n",
    "setwd(\"~/dropbox/1f.ἡἔρις,κ/1.ontology\")\n",
    "\n",
    "# Recode donor relationship and race variables\n",
    "data <- data %>%\n",
    "  mutate(related = ifelse(don_relation %in% 1:4, 1, 0),\n",
    "         racecat = case_when(\n",
    "           don_race == 8 ~ 1,\n",
    "           don_race == 16 ~ 2,\n",
    "           don_race == 2000 ~ 3,\n",
    "           don_race == 64 ~ 4,\n",
    "           TRUE ~ 5\n",
    "         ))\n",
    "\n",
    "# Create year and month variables based on donation recovery date\n",
    "data$year <- format(as.Date(data$don_recov_dt), \"%Y\")\n",
    "data$month <- format(as.Date(data$don_recov_dt), \"%m\")\n",
    "\n",
    "# Tabulate donations by year\n",
    "table(data$year)\n",
    "\n",
    "# Tabulate month and calculate the June-to-January ratio\n",
    "month_tab <- table(data$month)\n",
    "freq_jan <- month_tab[\"01\"]\n",
    "freq_jun <- month_tab[\"06\"]\n",
    "ratio <- freq_jun / freq_jan\n",
    "print(ratio)\n",
    "\n",
    "# Create a constant variable 'count' and recode 'month' for summer vs. rest of the year\n",
    "data <- data %>%\n",
    "  mutate(count = 1,\n",
    "         summer = ifelse(month %in% c(\"06\", \"07\", \"08\"), 1, 0))\n",
    "\n",
    "# Collapsing the data (your comment indicated this as more efficient than \"collapse\")\n",
    "if (0) {\n",
    "  data <- data %>%\n",
    "    group_by(year, month, summer, related) %>%\n",
    "    summarise(n_month = n())\n",
    "}\n",
    "\n",
    "# Running single-level Poisson model first to get starting estimates\n",
    "poisson_model <- glm(donations ~ summer + related, family = poisson, data = data)\n",
    "start_vals <- coef(poisson_model)\n",
    "\n",
    "# Placeholder for setting initial estimates\n",
    "# To check the starting values before using them\n",
    "print(start_vals)\n",
    "\n",
    "# Run multilevel Poisson model using initial estimates (measured in iterations)\n",
    "# Uncomment this block if you want to proceed with the multilevel Poisson model\n",
    "if (0) {\n",
    "  mepoisson_model <- glmer(donations ~ (1 | summer), family = poisson, data = data, start = start_vals)\n",
    "}\n",
    "\n",
    "# Close the log\n",
    "sink()\n",
    "```\n",
    "\n",
    "This should accomplish the same tasks as your original Stata code. Let me know if you have any questions or need further clarification.\n",
    "\n",
    "# 17\n",
    "\n",
    "```bash \n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Parameters required by the script\n",
    "read -p \"Enter your GitHub username (e.g., abikesa): \" GITHUB_USER\n",
    "read -p \"Enter your GitHub repository name (e.g., nabongo): \" GITHUB_REPO\n",
    "read -p \"Enter your email address (e.g., abikesa.sh@gmail.com): \" EMAIL_ADDRESS\n",
    "read -p \"Enter your root directory (e.g., ~/Dropbox/1f.ἡἔρις,κ/1.ontology): \" ROOT_DIR\n",
    "read -p \"Enter the name of the local directory to be created within the root directory (e.g., charles): \" LOCAL_DIR\n",
    "read -p \"Enter the SSH key name (e.g., id_charlesnabongo): \" SSH_KEYNAME\n",
    "read -p \"Enter your git commit message (e.g., automated abikesa_fromfolks.sh script): \" GIT_COMMIT_MESSAGE\n",
    "\n",
    "# Folked from some_repo to GITHUB_USER/GITHUB_REPO\n",
    "cd $(eval echo $ROOT_DIR)\n",
    "git clone \"https://github.com/$GITHUB_USER/$GITHUB_REPO\"\n",
    "cp -r \"$GITHUB_REPO/*\" \"$LOCAL_DIR\"\n",
    "jb build \"$LOCAL_DIR\"\n",
    "cp -r \"$LOCAL_DIR/*\" \"$GITHUB_REPO\"\n",
    "cd \"$GITHUB_REPO\"\n",
    "git add .\n",
    "git commit -m \"$GIT_COMMIT_MESSAGE\"\n",
    "\n",
    "# Overcoming the error: no sufficient permissions\n",
    "git remote -v\n",
    "git remote set-url origin \"git@github.com:$GITHUB_USER/$GITHUB_REPO\"\n",
    "git config user.name \"$GITHUB_USER\"\n",
    "git config user.email \"$EMAIL_ADDRESS\"\n",
    "git checkout main\n",
    "\n",
    "# Check if SSH keys already exist, and if not, generate a new one\n",
    "read -p \"Enter your SSH key name (e.g., id_charlesnabongo, not ~/.ssh/id_charlesnabongo): \" SSH_KEYNAME\n",
    "SSH_KEYPATH=\"$HOME/.ssh/$SSH_KEYNAME\"\n",
    "\n",
    "if [ ! -f \"$SSH_KEYPATH\" ]; then\n",
    "  ssh-keygen -t ed25519 -C \"$EMAIL_ADDRESS\" -f $SSH_KEYPATH\n",
    "fi\n",
    "\n",
    "cat \"$SSH_KEYPATH.pub\"\n",
    "echo \"Please manually add the above SSH public key to your GitHub account's SSH keys.\"\n",
    "read -p \"Once you have added the SSH key to your GitHub account, press Enter to continue...\"\n",
    "eval \"$(ssh-agent -s)\"\n",
    "ssh-add -D\n",
    "ssh-add $SSH_KEYPATH\n",
    "chmod 600 $SSH_KEYPATH\n",
    "\n",
    "# Configure the remote URL with SSH\n",
    "git remote set-url origin \"git@github.com:$GITHUB_USER/$GITHUB_REPO\"\n",
    "\n",
    "# Push changes\n",
    "git push -u origin main\n",
    "ghp-import -n -p -f _build/html\n",
    "rm -rf \"$GITHUB_REPO\"\n",
    "echo \"jb content updated & pushed to $GITHUB_USER/$GITHUB_REPO repository!\"\n",
    "\n",
    "\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
